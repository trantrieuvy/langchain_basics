{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai \n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().date()\n",
    "target_time = datetime.date(2024, 6, 12)\n",
    "if current_time > target_time:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L'Or Espresso Café \\n</td>\n",
       "      <td>Je trouve le goût médiocre. La mousse ne tient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hervidor de Agua Eléctrico</td>\n",
       "      <td>Está lu bonita calienta muy rápido, es muy fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product  \\\n",
       "0        Queen Size Sheet Set   \n",
       "1      Waterproof Phone Pouch   \n",
       "2         Luxury Air Mattress   \n",
       "3              Pillows Insert   \n",
       "4     Milk Frother Handheld\\n   \n",
       "5       L'Or Espresso Café \\n   \n",
       "6  Hervidor de Agua Eléctrico   \n",
       "\n",
       "                                              Review  \n",
       "0  I ordered a king size set. My only criticism w...  \n",
       "1  I loved the waterproof sac, although the openi...  \n",
       "2  This mattress had a small hole in the top of i...  \n",
       "3  This is the best throw pillow fillers on Amazo...  \n",
       "4   I loved this product. But they only seem to l...  \n",
       "5  Je trouve le goût médiocre. La mousse ne tient...  \n",
       "6  Está lu bonita calienta muy rápido, es muy fun...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=llm_model, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is name of the biggest competitor that makes {product}?, give name only, no other text.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Waterproof Phone Pouch'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = df[\"Product\"].iloc[1]\n",
    "product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is name of the biggest competitor that makes Waterproof Phone Pouch?, give name only, no other text.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'JOTO'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(product=product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=llm_model, temperature=0.9)\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is name of the biggest competitor that makes {product}?, give name only, no other text.\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt,\n",
    "    verbose=True\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sencond_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Write a 20 words discription for the following competitors: {competitors}\"\"\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=sencond_prompt,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    # verbose=True\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is name of the biggest competitor that makes Waterproof Phone Pouch?, give name only, no other text.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Write a 20 words discription for the following competitors: JOTO\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'JOTO offers innovative office solutions with a focus on organization and productivity, perfect for professionals seeking efficiency and style.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I loved the waterproof sac, although the opening was made of a hard plastic. I don’t know if that would break easily. But I couldn’t turn my phone on, once it was in the pouch.',\n",
       " 'vietnamese_review': 'Tôi rất thích túi chống nước, mặc dù phần mở của nó được làm từ nhựa cứng. Tôi không biết liệu điều đó có dễ bị hỏng không. Nhưng khi điện thoại của tôi ở trong túi, tôi không thể bật máy được.',\n",
       " 'language': 'This review is in Vietnamese.',\n",
       " 'vietnamese_summary': 'The reviewer likes the waterproof bag, but is concerned about the hard plastic opening and the inability to use their phone while it is inside the bag.',\n",
       " 'follow_up_response': 'Cảm ơn bạn đã chia sẻ ý kiến về túi chống nước. Chúng tôi rất vui khi biết bạn thích sản phẩm này. Chúng tôi sẽ tiếp thu ý kiến của bạn về việc mở túi bằng nhựa cứng và khả năng không sử dụng điện thoại khi đưa vào túi. Chúng tôi sẽ cố gắng cải thiện sản phẩm để đáp ứng nhu cầu của quý khách hàng. Cảm ơn bạn đã quan tâm và ủng hộ sản phẩm của chúng tôi.'}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=llm_model, temperature=0.9)\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to Vietnamese: {review}\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=first_prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"vietnamese_review\"\n",
    ")   \n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"What language is the following review: {vietnamese_review}\"\"\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=second_prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"language\"\n",
    ")\n",
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Summarize the following review in one sentence: {vietnamese_review} in {language} language\"\"\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=third_prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"vietnamese_summary\"\n",
    ")\n",
    "\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {vietnamese_summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=fourth_prompt,\n",
    "    verbose=False,\n",
    "    output_key=\"follow_up_response\"\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"vietnamese_review\", \"language\", \"vietnamese_summary\", \"follow_up_response\"],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "review = df[\"Review\"].iloc[1]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience_template = \"\"\"\n",
    "You are a talented data scientist. You are great at answering data science related questions in a concise and easy to understand manner. \\\n",
    "When you don't know the answer, you will say 'I don't know'. \\\n",
    "You will not make up answers. \\ \n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "biology_template = \"\"\"\n",
    "You are a talented biology expert. You are great at answering biology related questions in a concise and easy to understand manner. \\\n",
    "When you don't know the answer, you will say 'I don't know'. \\\n",
    "You will not make up answers. \\ \n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "law_template = \"\"\"\n",
    "You are a talented lawyer. You are great at answering law related questions in a concise and easy to understand manner. \\\n",
    "When you don't know the answer, you will say 'I don't know'. \\\n",
    "You will not make up answers. \\ \n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "software_template = \"\"\"\n",
    "You are a talented software engineer. You are great at answering software engineering related questions in a concise and easy to understand manner. \\\n",
    "When you don't know the answer, you will say 'I don't know'. \\\n",
    "You will not make up answers. \\\n",
    "Here is a question:\n",
    "{input} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\" : \"Data Science\",\n",
    "        \"description\": \"Good for answering data science related questions\",\n",
    "        \"template\": datascience_template\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"Biology\",\n",
    "        \"description\": \"Good for answering biology related questions\",\n",
    "        \"template\": biology_template\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"Law\",\n",
    "        \"description\": \"Good for answering law related questions\",\n",
    "        \"template\": law_template\n",
    "    },\n",
    "    {\n",
    "        \"name\" : \"Software Engineering\",\n",
    "        \"description\": \"Good for answering software engineering related questions\",\n",
    "        \"template\": software_template        \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for prompt_info in prompt_infos:\n",
    "    name = prompt_info[\"name\"]\n",
    "    prompt_template = prompt_info[\"template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=False\n",
    "    )\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data Science': LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"\\nYou are a talented data scientist. You are great at answering data science related questions in a concise and easy to understand manner. When you don't know the answer, you will say 'I don't know'. You will not make up answers. \\\\ \\nHere is a question:\\n{input}\"), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027738AC22B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027738AE2AF0>, root_client=<openai.OpenAI object at 0x0000027738B57370>, root_async_client=<openai.AsyncOpenAI object at 0x0000027738AC23A0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'Biology': LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"\\nYou are a talented biology expert. You are great at answering biology related questions in a concise and easy to understand manner. When you don't know the answer, you will say 'I don't know'. You will not make up answers. \\\\ \\nHere is a question:\\n{input}\"), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027738AC22B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027738AE2AF0>, root_client=<openai.OpenAI object at 0x0000027738B57370>, root_async_client=<openai.AsyncOpenAI object at 0x0000027738AC23A0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'Law': LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"\\nYou are a talented lawyer. You are great at answering law related questions in a concise and easy to understand manner. When you don't know the answer, you will say 'I don't know'. You will not make up answers. \\\\ \\nHere is a question:\\n{input}\"), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027738AC22B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027738AE2AF0>, root_client=<openai.OpenAI object at 0x0000027738B57370>, root_async_client=<openai.AsyncOpenAI object at 0x0000027738AC23A0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'Software Engineering': LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"\\nYou are a talented software engineer. You are great at answering software engineering related questions in a concise and easy to understand manner. When you don't know the answer, you will say 'I don't know'. You will not make up answers. Here is a question:\\n{input} \"), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027738AC22B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027738AE2AF0>, root_client=<openai.OpenAI object at 0x0000027738B57370>, root_async_client=<openai.AsyncOpenAI object at 0x0000027738AC23A0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=default_prompt,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000027738AC22B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000027738AE2AF0>, root_client=<openai.OpenAI object at 0x0000027738B57370>, root_async_client=<openai.AsyncOpenAI object at 0x0000027738AC23A0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in Data Science: Good for answering data science related questions\n",
      "Biology: Good for answering biology related questions\n",
      "Law: Good for answering law related questions\n",
      "Software Engineering: Good for answering software engineering related questions\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: The value of “destination” MUST match one of the candidate prompts listed below.If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "Data Science: Good for answering data science related questions\n",
      "Biology: Good for answering biology related questions\n",
      "Law: Good for answering law related questions\n",
      "Software Engineering: Good for answering software engineering related questions\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (remember to include the ```json)>>\n"
     ]
    }
   ],
   "source": [
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "Data Science: {'input': 'What is a Huber Regression Model?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A Huber regression model is a type of robust regression model that is less sensitive to outliers in the data compared to traditional least squares regression. It combines the best of both worlds by using a squared loss for data points close to the regression line and a linear loss for data points that are further away, effectively reducing the impact of outliers on the model's performance. This makes the Huber regression model more robust and reliable when dealing with noisy data.\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is a Huber Regression Model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "Biology: {'input': 'What is single cell RNA sequencing?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Single-cell RNA sequencing is a technique used to analyze the gene expression profile of individual cells. It allows researchers to study the heterogeneity of gene expression within a population of cells, providing insights into cell types, cell states, and cellular functions. This technique has revolutionized our understanding of cellular diversity and has been instrumental in advancing fields such as developmental biology, cancer research, and immunology.'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is single cell RNA sequencing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tom and Jerry is a classic animated television series created by William Hanna and Joseph Barbera. It follows the comedic antics of a cat named Tom and a mouse named Jerry as they engage in a never-ending rivalry. The show first premiered in 1940 and has since become a beloved and iconic part of pop culture.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is Tom and Jerry?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
